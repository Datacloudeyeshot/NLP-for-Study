{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Pr(sentence) = Pr(w_1 \\cdot w_2 \\cdots w_n) = \\prod \\frac{count(w_i, w_{i+1})}{count(w_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = '/Users/Lenovo/Desktop/Data/train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = open(corpus.strip(), encoding='UTF-8').read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入jieba分词库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对文本进行切分分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut():\n",
    "    word_1 = FILE.replace('++$++','')\n",
    "    word_2 = word_1.replace(' ','')\n",
    "    word_3 = word_2.replace('\\n','')\n",
    "    return list(jieba.cut(word_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成2-gram模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS = cut()\n",
    "words_count = Counter(TOKENS)\n",
    "_2_gram_words = [TOKENS[i] + TOKENS[i+1] for i in range(len(TOKENS)- 1)  ]\n",
    "_2_gram_word_counts = Counter(_2_gram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('-', 13317),\n",
       " ('?', 12876),\n",
       " ('？', 12736),\n",
       " ('insurance', 11158),\n",
       " ('保险', 5013),\n",
       " ('的', 3220),\n",
       " ('人寿保险', 2962),\n",
       " ('什么', 2677),\n",
       " ('吗', 2479),\n",
       " ('是', 2344),\n",
       " ('我', 2053),\n",
       " ('是否', 1862),\n",
       " ('可以', 1704),\n",
       " ('健康', 1513),\n",
       " ('如何', 1294),\n",
       " ('医疗保险', 1269),\n",
       " ('多少', 1252),\n",
       " ('汽车保险', 1189),\n",
       " ('在', 913),\n",
       " ('覆盖', 848)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 客户语言定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"\"\"\n",
    "user = 自己 时间 地点 事件\n",
    "自己 = 我 | 俺 | 我们 \n",
    "时间 = 刚刚 | 今天 | 一小时前\n",
    "地点 = 在路上 | 在医院 \n",
    "事件 = 出车祸 | 做手术 | 摔跤\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 客服语言定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号客服 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 投保 具体业务\n",
    "投保 = 购买 | 咨询 | 退订\n",
    "具体业务 = 报案 | 意外保险 | 汽车保险 | 医疗保险\n",
    "结尾 = 吗？\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_generation_by_gram(grammar_str, target, stmt_split='=', or_split='|'):\n",
    "    rules = dict()  #数据用字典存储，对象为字典\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line: continue\n",
    "#         使单词分开\n",
    "        stmt, expression = line.split(stmt_split)   \n",
    "#         去除单词后的空格\n",
    "        rules[stmt.strip()] = expression.split(or_split)\n",
    "        generated = generate (rules, target = target)\n",
    "    return generated   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依据语法生成一句话"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(grammar_rule, target):\n",
    "    if target in grammar_rule: \n",
    "#         取得对应的数据集grammer_rule\n",
    "        words = grammar_rule[target] \n",
    "#     依据语法随机生成一句话\n",
    "        word = random.choice(words) \n",
    "        return ''.join(generate(grammar_rule, target=c.strip()) for c in word.split()) #去掉首尾空格\n",
    "    else:\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好我是7593号客服,您需要购买报案吗？\n",
      "我们今天在路上做手术\n",
      "先生,你好我是375号客服,请问你要咨询汽车险吗？\n",
      "我今天在医院做手术\n",
      "你好我是4638号客服,您需要咨询意外险吗？\n",
      "俺今天在路上做手术\n",
      "女士,您好我是2号客服,您需要购买报案吗？\n",
      "俺今天在医院摔跤\n",
      "您好我是7542号客服,请问你要咨询意外险吗？\n",
      "我们今天在医院摔跤\n"
     ]
    }
   ],
   "source": [
    "# 语句生成测试\n",
    "for i in range(5):\n",
    "    print(get_generation_by_gram(host,target='host')) \n",
    "    print(get_generation_by_gram(user,target='user'))\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gram模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1_gram_count(word):\n",
    "    if word in words_count: return words_count[word]\n",
    "    else:\n",
    "        return words_count.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2_gram_count(word):\n",
    "    if word in _2_gram_word_counts: return _2_gram_word_counts[word]\n",
    "    else:\n",
    "        return _2_gram_word_counts.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_count(word, wc):\n",
    "    if word in wc: return wc[word]\n",
    "    else:\n",
    "        return wc.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_gram_model(sentence):\n",
    "    # 2-gram langauge model\n",
    "    tokens = list(jieba.cut(sentence))\n",
    "    \n",
    "    probability = 1\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "        \n",
    "        _two_gram_c = get_gram_count(word+next_word, _2_gram_word_counts)\n",
    "        _one_gram_c = get_gram_count(next_word, words_count)\n",
    "        pro =  _two_gram_c / _one_gram_c\n",
    "        \n",
    "        probability *= pro\n",
    "    \n",
    "    return probability  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.776893279789616e-05"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('怎么赔偿人寿保险')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_say(grammar_str: str, target, stmt_split='=', or_split='|'):\n",
    "    rules = dict()\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line:continue\n",
    "        stmt, expr = line.split(stmt_split)\n",
    "        rules[stmt.strip()] = expr.split(or_split)\n",
    "    generated = generate(rules, target=target)\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(grammar_str: str, target, n : int):\n",
    "    sentences = list()\n",
    "    for i in range(n):\n",
    "        sentences.append(generate_say(grammar_str, target))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(grammar_str: str, target: str, n : int):\n",
    "    sentance = list()\n",
    "    sentance_list = generate_n(grammar_str,target,5)\n",
    "    for i in sentance_list:\n",
    "        sentance.append([i, two_gram_model(i)])\n",
    "    sentance_sort_list = sorted(sentance,key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sentance_sort_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['你好我是39号客服,您需要咨询报案吗？', 1.148284360096569e-16]\n",
      "['我刚刚在路上出车祸', 0.0007301935012778385]\n",
      "\n",
      "['小朋友,您好我是96号客服,请问你要咨询报案吗？', 6.579121050569782e-19]\n",
      "['我们今天在路上出车祸', 0.0007301935012778385]\n",
      "\n",
      "['您好我是4582号客服,请问你要退订医疗险吗？', 2.9606044727564026e-17]\n",
      "['俺今天在路上出车祸', 0.0007301935012778385]\n",
      "\n",
      "['女士,您好我是83号客服,您需要退订医疗险吗？', 2.55174302243682e-18]\n",
      "['我刚刚在路上做手术', 0.0007301935012778385]\n",
      "\n",
      "['你好我是4号客服,您需要咨询报案吗？', 5.741421800482845e-17]\n",
      "['我们今天在路上做手术', 0.0007301935012778385]\n",
      "\n",
      "['小朋友,你好我是5号客服,请问你要退订医疗险吗？', 6.579121050569782e-19]\n",
      "['我刚刚在路上摔跤', 0.0007301935012778385]\n",
      "\n",
      "['您好我是5号客服,您需要退订意外险吗？', 1.148284360096569e-16]\n",
      "['我刚刚在路上做手术', 0.0007301935012778385]\n",
      "\n",
      "['您好我是775号客服,您需要退订意外险吗？', 1.148284360096569e-16]\n",
      "['我刚刚在医院出车祸', 6.845564074479737e-05]\n",
      "\n",
      "['女士,您好我是5号客服,请问你要退订医疗险吗？', 6.579121050569782e-19]\n",
      "['我刚刚在路上出车祸', 0.0007301935012778385]\n",
      "\n",
      "['你好我是1号客服,请问你要退订报案吗？', 2.6914586115967292e-18]\n",
      "['我今天在路上出车祸', 0.0007301935012778385]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(generate_best(host,'host',5))\n",
    "    print(generate_best(user,'user',4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
